
Importing the Libraries and Dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
Data Collection and Processing
# Loading the data from csv file and create Pandas DataFrame
creditcard_data = pd.read_csv('creditcard.csv')
# print 5 rows of dataset.
creditcard_data.head()
Time	V1	V2	V3	V4	V5	V6	V7	V8	V9	...	V21	V22	V23	V24	V25	V26	V27	V28	Amount	Class
0	0.0	-1.359807	-0.072781	2.536347	1.378155	-0.338321	0.462388	0.239599	0.098698	0.363787	...	-0.018307	0.277838	-0.110474	0.066928	0.128539	-0.189115	0.133558	-0.021053	149.62	0
1	0.0	1.191857	0.266151	0.166480	0.448154	0.060018	-0.082361	-0.078803	0.085102	-0.255425	...	-0.225775	-0.638672	0.101288	-0.339846	0.167170	0.125895	-0.008983	0.014724	2.69	0
2	1.0	-1.358354	-1.340163	1.773209	0.379780	-0.503198	1.800499	0.791461	0.247676	-1.514654	...	0.247998	0.771679	0.909412	-0.689281	-0.327642	-0.139097	-0.055353	-0.059752	378.66	0
3	1.0	-0.966272	-0.185226	1.792993	-0.863291	-0.010309	1.247203	0.237609	0.377436	-1.387024	...	-0.108300	0.005274	-0.190321	-1.175575	0.647376	-0.221929	0.062723	0.061458	123.50	0
4	2.0	-1.158233	0.877737	1.548718	0.403034	-0.407193	0.095921	0.592941	-0.270533	0.817739	...	-0.009431	0.798278	-0.137458	0.141267	-0.206010	0.502292	0.219422	0.215153	69.99	0
5 rows × 31 columns

# print last 5 rows of dataset.
creditcard_data.tail()
Time	V1	V2	V3	V4	V5	V6	V7	V8	V9	...	V21	V22	V23	V24	V25	V26	V27	V28	Amount	Class
284802	172786.0	-11.881118	10.071785	-9.834783	-2.066656	-5.364473	-2.606837	-4.918215	7.305334	1.914428	...	0.213454	0.111864	1.014480	-0.509348	1.436807	0.250034	0.943651	0.823731	0.77	0
284803	172787.0	-0.732789	-0.055080	2.035030	-0.738589	0.868229	1.058415	0.024330	0.294869	0.584800	...	0.214205	0.924384	0.012463	-1.016226	-0.606624	-0.395255	0.068472	-0.053527	24.79	0
284804	172788.0	1.919565	-0.301254	-3.249640	-0.557828	2.630515	3.031260	-0.296827	0.708417	0.432454	...	0.232045	0.578229	-0.037501	0.640134	0.265745	-0.087371	0.004455	-0.026561	67.88	0
284805	172788.0	-0.240440	0.530483	0.702510	0.689799	-0.377961	0.623708	-0.686180	0.679145	0.392087	...	0.265245	0.800049	-0.163298	0.123205	-0.569159	0.546668	0.108821	0.104533	10.00	0
284806	172792.0	-0.533413	-0.189733	0.703337	-0.506271	-0.012546	-0.649617	1.577006	-0.414650	0.486180	...	0.261057	0.643078	0.376777	0.008797	-0.473649	-0.818267	-0.002415	0.013649	217.00	0
5 rows × 31 columns

# print number of rows and columns 
creditcard_data.shape
(284807, 31)
# print the information of dataset
creditcard_data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 284807 entries, 0 to 284806
Data columns (total 31 columns):
 #   Column  Non-Null Count   Dtype  
---  ------  --------------   -----  
 0   Time    284807 non-null  float64
 1   V1      284807 non-null  float64
 2   V2      284807 non-null  float64
 3   V3      284807 non-null  float64
 4   V4      284807 non-null  float64
 5   V5      284807 non-null  float64
 6   V6      284807 non-null  float64
 7   V7      284807 non-null  float64
 8   V8      284807 non-null  float64
 9   V9      284807 non-null  float64
 10  V10     284807 non-null  float64
 11  V11     284807 non-null  float64
 12  V12     284807 non-null  float64
 13  V13     284807 non-null  float64
 14  V14     284807 non-null  float64
 15  V15     284807 non-null  float64
 16  V16     284807 non-null  float64
 17  V17     284807 non-null  float64
 18  V18     284807 non-null  float64
 19  V19     284807 non-null  float64
 20  V20     284807 non-null  float64
 21  V21     284807 non-null  float64
 22  V22     284807 non-null  float64
 23  V23     284807 non-null  float64
 24  V24     284807 non-null  float64
 25  V25     284807 non-null  float64
 26  V26     284807 non-null  float64
 27  V27     284807 non-null  float64
 28  V28     284807 non-null  float64
 29  Amount  284807 non-null  float64
 30  Class   284807 non-null  int64  
dtypes: float64(30), int64(1)
memory usage: 67.4 MB
# find the number of missing values in each column
creditcard_data.isnull().sum()
Time      0
V1        0
V2        0
V3        0
V4        0
V5        0
V6        0
V7        0
V8        0
V9        0
V10       0
V11       0
V12       0
V13       0
V14       0
V15       0
V16       0
V17       0
V18       0
V19       0
V20       0
V21       0
V22       0
V23       0
V24       0
V25       0
V26       0
V27       0
V28       0
Amount    0
Class     0
dtype: int64
# Categorise the legal and fraud Transactions 
creditcard_data['Class'].value_counts()
0    284315
1       492
Name: Class, dtype: int64
As the data is highly biased, we separate some data from Analysis
legal = creditcard_data[creditcard_data.Class == 0]
fraud = creditcard_data[creditcard_data.Class == 1]
legal.shape
(284315, 31)
fraud.shape
(492, 31)
# Performing statistics for each column
legal.Amount.describe()
count    284315.000000
mean         88.291022
std         250.105092
min           0.000000
25%           5.650000
50%          22.000000
75%          77.050000
max       25691.160000
Name: Amount, dtype: float64
fraud.Amount.describe()
count     492.000000
mean      122.211321
std       256.683288
min         0.000000
25%         1.000000
50%         9.250000
75%       105.890000
max      2125.870000
Name: Amount, dtype: float64
# Compare the Values for both transactions 
creditcard_data.groupby('Class').mean()
Time	V1	V2	V3	V4	V5	V6	V7	V8	V9	...	V20	V21	V22	V23	V24	V25	V26	V27	V28	Amount
Class																					
0	94838.202258	0.008258	-0.006271	0.012171	-0.007860	0.005453	0.002419	0.009637	-0.000987	0.004467	...	-0.000644	-0.001235	-0.000024	0.000070	0.000182	-0.000072	-0.000089	-0.000295	-0.000131	88.291022
1	80746.806911	-4.771948	3.623778	-7.033281	4.542029	-3.151225	-1.397737	-5.568731	0.570636	-2.581123	...	0.372319	0.713588	0.014049	-0.040308	-0.105130	0.041449	0.051648	0.170575	0.075667	122.211321
2 rows × 30 columns

Under-Sampling
# Build a Sample Dataset contain similar Distribution  for Legal Transactions and Fraud Transaction 
legal_sample = legal.sample(n=492)
# concat the two DataFrame
new_creditcard_data = pd.concat([legal_sample, fraud], axis = 0)
new_creditcard_data.head()
Time	V1	V2	V3	V4	V5	V6	V7	V8	V9	...	V21	V22	V23	V24	V25	V26	V27	V28	Amount	Class
270125	163929.0	-0.737149	0.363992	0.109856	-2.281539	0.159357	-0.001035	-0.387543	0.659386	-1.163323	...	0.356318	0.739558	-0.305717	-0.027153	0.066595	-0.194425	-0.001088	0.078123	19.99	0
268180	163097.0	2.103619	-0.366501	-1.703807	-1.441742	-0.107320	-1.683070	0.390454	-0.507857	1.443158	...	-0.112738	-0.068511	0.130685	-0.039551	0.131381	-0.309419	-0.024941	-0.061746	18.02	0
152892	97537.0	2.007775	-0.330731	-1.331810	0.177638	0.297419	0.045130	-0.239334	-0.077565	2.260090	...	-0.050293	0.286106	-0.012560	0.183622	0.213410	0.134122	-0.069872	-0.073986	29.40	0
217475	140903.0	0.157764	1.041504	-0.491134	-0.592337	1.027599	-0.708620	0.993574	-0.110814	-0.067471	...	-0.351827	-0.856119	0.071764	0.375643	-0.384079	0.124607	0.222738	0.082851	5.38	0
171628	120740.0	1.940435	0.175985	-1.615537	1.416568	0.493786	-1.103092	0.791571	-0.503635	0.004553	...	0.092475	0.453677	-0.057710	0.029384	0.483978	-0.498639	-0.015095	-0.051265	54.75	0
5 rows × 31 columns

new_creditcard_data.tail()
Time	V1	V2	V3	V4	V5	V6	V7	V8	V9	...	V21	V22	V23	V24	V25	V26	V27	V28	Amount	Class
279863	169142.0	-1.927883	1.125653	-4.518331	1.749293	-1.566487	-2.010494	-0.882850	0.697211	-2.064945	...	0.778584	-0.319189	0.639419	-0.294885	0.537503	0.788395	0.292680	0.147968	390.00	1
280143	169347.0	1.378559	1.289381	-5.004247	1.411850	0.442581	-1.326536	-1.413170	0.248525	-1.127396	...	0.370612	0.028234	-0.145640	-0.081049	0.521875	0.739467	0.389152	0.186637	0.76	1
280149	169351.0	-0.676143	1.126366	-2.213700	0.468308	-1.120541	-0.003346	-2.234739	1.210158	-0.652250	...	0.751826	0.834108	0.190944	0.032070	-0.739695	0.471111	0.385107	0.194361	77.89	1
281144	169966.0	-3.113832	0.585864	-5.399730	1.817092	-0.840618	-2.943548	-2.208002	1.058733	-1.632333	...	0.583276	-0.269209	-0.456108	-0.183659	-0.328168	0.606116	0.884876	-0.253700	245.00	1
281674	170348.0	1.991976	0.158476	-2.583441	0.408670	1.151147	-0.096695	0.223050	-0.068384	0.577829	...	-0.164350	-0.295135	-0.072173	-0.450261	0.313267	-0.289617	0.002988	-0.015309	42.53	1
5 rows × 31 columns

new_creditcard_data['Class'].value_counts()
0    492
1    492
Name: Class, dtype: int64
new_creditcard_data.groupby('Class').mean()
Time	V1	V2	V3	V4	V5	V6	V7	V8	V9	...	V20	V21	V22	V23	V24	V25	V26	V27	V28	Amount
Class																					
0	96279.101626	-0.000121	-0.047602	-0.026038	-0.063768	0.090866	0.003265	0.041157	0.020726	-0.053113	...	-0.006673	0.019732	-0.015904	-0.044521	-0.022358	-0.013387	0.003002	0.026844	0.008977	92.548780
1	80746.806911	-4.771948	3.623778	-7.033281	4.542029	-3.151225	-1.397737	-5.568731	0.570636	-2.581123	...	0.372319	0.713588	0.014049	-0.040308	-0.105130	0.041449	0.051648	0.170575	0.075667	122.211321
2 rows × 30 columns

Splitting the new data into Features and Target
x = new_creditcard_data.drop(columns = 'Class',axis = 1)
y = new_creditcard_data['Class']
x.head()
Time	V1	V2	V3	V4	V5	V6	V7	V8	V9	...	V20	V21	V22	V23	V24	V25	V26	V27	V28	Amount
270125	163929.0	-0.737149	0.363992	0.109856	-2.281539	0.159357	-0.001035	-0.387543	0.659386	-1.163323	...	0.015762	0.356318	0.739558	-0.305717	-0.027153	0.066595	-0.194425	-0.001088	0.078123	19.99
268180	163097.0	2.103619	-0.366501	-1.703807	-1.441742	-0.107320	-1.683070	0.390454	-0.507857	1.443158	...	-0.165214	-0.112738	-0.068511	0.130685	-0.039551	0.131381	-0.309419	-0.024941	-0.061746	18.02
152892	97537.0	2.007775	-0.330731	-1.331810	0.177638	0.297419	0.045130	-0.239334	-0.077565	2.260090	...	-0.209375	-0.050293	0.286106	-0.012560	0.183622	0.213410	0.134122	-0.069872	-0.073986	29.40
217475	140903.0	0.157764	1.041504	-0.491134	-0.592337	1.027599	-0.708620	0.993574	-0.110814	-0.067471	...	0.044914	-0.351827	-0.856119	0.071764	0.375643	-0.384079	0.124607	0.222738	0.082851	5.38
171628	120740.0	1.940435	0.175985	-1.615537	1.416568	0.493786	-1.103092	0.791571	-0.503635	0.004553	...	-0.126675	0.092475	0.453677	-0.057710	0.029384	0.483978	-0.498639	-0.015095	-0.051265	54.75
5 rows × 30 columns

y.head()
270125    0
268180    0
152892    0
217475    0
171628    0
Name: Class, dtype: int64
Split the new dataset for training and testing
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = y, random_state = 2)
(x.shape, x_train.shape, x_test.shape)
((984, 30), (787, 30), (197, 30))
(y.shape, y_train.shape, y_test.shape)
((984,), (787,), (197,))
Model Training
model = LogisticRegression()
# training the model with training data
model.fit(x_train, y_train)
LogisticRegression()
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
Model Evaluation
x_train_prediction = model.predict(x_train)
print(x_train_prediction)
[0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1
 0 0 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0
 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1
 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0
 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0
 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1
 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0
 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1
 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0
 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0
 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0
 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1
 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0
 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0
 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0
 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1
 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1
 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0
 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0
 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1
 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0
 1 1 0 1 0 0 0 1 1 0]
x_test_prediction = model.predict(x_test)
print(x_test_prediction)
[1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 0
 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0
 1 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0
 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0
 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1
 0 0 0 0 0 1 1 0 1 1 1 0]
# Accuracy on training data
accuracy_of_train_data = accuracy_score(x_train_prediction, y_train)
print('Accuracy Score of Training data:',accuracy_of_train_data)
# Accuracy on test data
accuracy_of_test_data = accuracy_score(x_test_prediction, y_test)
print('Accuracy Score of Testing data:',accuracy_of_test_data)
Accuracy Score of Training data: 0.9351969504447268
Accuracy Score of Testing data: 0.9289340101522843
# Precision on training data
precision_of_train_data = precision_score(x_train_prediction, y_train)
print('Precision Score of Training data:',precision_of_train_data)
# Precision on test data
precision_of_test_data = precision_score(x_test_prediction, y_test)
print('Precision Score of Testing data:',precision_of_test_data)
Precision Score of Training data: 0.9187817258883249
Precision Score of Testing data: 0.8775510204081632
# Recall on training data
recall_of_train_data = recall_score(x_train_prediction, y_train)
print('Recall Score of Training data:',recall_of_train_data)
# Recall on test data
recall_of_test_data = recall_score(x_test_prediction, y_test)
print('Recall Score of Testing data:',recall_of_test_data)
Recall Score of Training data: 0.9501312335958005
Recall Score of Testing data: 0.9772727272727273
# f1-score on training data
f1score_of_train_data = f1_score(x_train_prediction, y_train)
print('F1-score Score of Training data:',f1score_of_train_data)
# Accuracy on test data
f1score_of_test_data = f1_score(x_test_prediction, y_test)
print('F1-score Score of Testing data:',f1score_of_test_data)
F1-score Score of Training data: 0.9341935483870968
F1-score Score of Testing data: 0.9247311827956989
 
